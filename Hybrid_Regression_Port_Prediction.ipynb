{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices(physical_devices[1], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  print('Visible devices:', visible_devices)\n",
    "except:\n",
    "  print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy.random as rnd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import csv\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import autokeras as ak\n",
    "from autokeras import StructuredDataClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D, AveragePooling1D\n",
    "from keras.backend import clear_session\n",
    "import optuna\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data and splitting the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_ports = 5\n",
    "threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset pseudo random generator to a known value so that results are reproducible.\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059398</td>\n",
       "      <td>0.183232</td>\n",
       "      <td>1.080309</td>\n",
       "      <td>2.614261</td>\n",
       "      <td>4.584051</td>\n",
       "      <td>6.749800</td>\n",
       "      <td>8.862194</td>\n",
       "      <td>10.691750</td>\n",
       "      <td>12.054431</td>\n",
       "      <td>12.830632</td>\n",
       "      <td>...</td>\n",
       "      <td>23.051142</td>\n",
       "      <td>21.468345</td>\n",
       "      <td>19.913783</td>\n",
       "      <td>18.554710</td>\n",
       "      <td>17.555975</td>\n",
       "      <td>17.061368</td>\n",
       "      <td>17.174608</td>\n",
       "      <td>17.942390</td>\n",
       "      <td>19.342121</td>\n",
       "      <td>21.276428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.778188</td>\n",
       "      <td>30.222646</td>\n",
       "      <td>36.359023</td>\n",
       "      <td>41.707733</td>\n",
       "      <td>45.852210</td>\n",
       "      <td>48.479536</td>\n",
       "      <td>49.409203</td>\n",
       "      <td>48.607128</td>\n",
       "      <td>46.183609</td>\n",
       "      <td>42.376109</td>\n",
       "      <td>...</td>\n",
       "      <td>9.341135</td>\n",
       "      <td>7.158377</td>\n",
       "      <td>5.033784</td>\n",
       "      <td>3.273157</td>\n",
       "      <td>2.148745</td>\n",
       "      <td>1.864735</td>\n",
       "      <td>2.530243</td>\n",
       "      <td>4.143316</td>\n",
       "      <td>6.588110</td>\n",
       "      <td>9.645830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.832415</td>\n",
       "      <td>7.090963</td>\n",
       "      <td>6.494046</td>\n",
       "      <td>6.230958</td>\n",
       "      <td>6.451343</td>\n",
       "      <td>7.240751</td>\n",
       "      <td>8.604879</td>\n",
       "      <td>10.464771</td>\n",
       "      <td>12.663762</td>\n",
       "      <td>14.985351</td>\n",
       "      <td>...</td>\n",
       "      <td>7.779974</td>\n",
       "      <td>4.201972</td>\n",
       "      <td>1.725694</td>\n",
       "      <td>0.367445</td>\n",
       "      <td>0.015321</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>1.393285</td>\n",
       "      <td>2.525027</td>\n",
       "      <td>3.553824</td>\n",
       "      <td>4.244767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.709786</td>\n",
       "      <td>3.666249</td>\n",
       "      <td>4.566536</td>\n",
       "      <td>5.313171</td>\n",
       "      <td>5.822715</td>\n",
       "      <td>6.036252</td>\n",
       "      <td>5.926975</td>\n",
       "      <td>5.503941</td>\n",
       "      <td>4.811494</td>\n",
       "      <td>3.924439</td>\n",
       "      <td>...</td>\n",
       "      <td>6.793335</td>\n",
       "      <td>5.646034</td>\n",
       "      <td>4.686189</td>\n",
       "      <td>4.033345</td>\n",
       "      <td>3.794001</td>\n",
       "      <td>4.044375</td>\n",
       "      <td>4.815511</td>\n",
       "      <td>6.082973</td>\n",
       "      <td>7.762920</td>\n",
       "      <td>9.715667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.603570</td>\n",
       "      <td>7.808421</td>\n",
       "      <td>6.219126</td>\n",
       "      <td>6.101364</td>\n",
       "      <td>7.555183</td>\n",
       "      <td>10.500586</td>\n",
       "      <td>14.683635</td>\n",
       "      <td>19.702510</td>\n",
       "      <td>25.050711</td>\n",
       "      <td>30.172516</td>\n",
       "      <td>...</td>\n",
       "      <td>13.615531</td>\n",
       "      <td>16.162296</td>\n",
       "      <td>18.415918</td>\n",
       "      <td>20.167117</td>\n",
       "      <td>21.241454</td>\n",
       "      <td>21.520567</td>\n",
       "      <td>20.958055</td>\n",
       "      <td>19.587812</td>\n",
       "      <td>17.523448</td>\n",
       "      <td>14.948487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1          2          3          4          5   \\\n",
       "0   0.059398   0.183232   1.080309   2.614261   4.584051   6.749800   \n",
       "1  23.778188  30.222646  36.359023  41.707733  45.852210  48.479536   \n",
       "2   7.832415   7.090963   6.494046   6.230958   6.451343   7.240751   \n",
       "3   2.709786   3.666249   4.566536   5.313171   5.822715   6.036252   \n",
       "4  10.603570   7.808421   6.219126   6.101364   7.555183  10.500586   \n",
       "\n",
       "          6          7          8          9   ...         90         91  \\\n",
       "0   8.862194  10.691750  12.054431  12.830632  ...  23.051142  21.468345   \n",
       "1  49.409203  48.607128  46.183609  42.376109  ...   9.341135   7.158377   \n",
       "2   8.604879  10.464771  12.663762  14.985351  ...   7.779974   4.201972   \n",
       "3   5.926975   5.503941   4.811494   3.924439  ...   6.793335   5.646034   \n",
       "4  14.683635  19.702510  25.050711  30.172516  ...  13.615531  16.162296   \n",
       "\n",
       "          92         93         94         95         96         97  \\\n",
       "0  19.913783  18.554710  17.555975  17.061368  17.174608  17.942390   \n",
       "1   5.033784   3.273157   2.148745   1.864735   2.530243   4.143316   \n",
       "2   1.725694   0.367445   0.015321   0.452600   1.393285   2.525027   \n",
       "3   4.686189   4.033345   3.794001   4.044375   4.815511   6.082973   \n",
       "4  18.415918  20.167117  21.241454  21.520567  20.958055  19.587812   \n",
       "\n",
       "          98         99  \n",
       "0  19.342121  21.276428  \n",
       "1   6.588110   9.645830  \n",
       "2   3.553824   4.244767  \n",
       "3   7.762920   9.715667  \n",
       "4  17.523448  14.948487  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados = pd.read_csv('./classical_channel_samples_SINR.csv', header=None)\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000\n",
      "(1000000, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.003086</td>\n",
       "      <td>10.004461</td>\n",
       "      <td>10.005724</td>\n",
       "      <td>10.006757</td>\n",
       "      <td>10.007477</td>\n",
       "      <td>10.007848</td>\n",
       "      <td>1.000789e+01</td>\n",
       "      <td>10.007660</td>\n",
       "      <td>10.007274</td>\n",
       "      <td>10.006859</td>\n",
       "      <td>...</td>\n",
       "      <td>10.013202</td>\n",
       "      <td>10.014060</td>\n",
       "      <td>10.014483</td>\n",
       "      <td>10.014552</td>\n",
       "      <td>10.014383</td>\n",
       "      <td>10.014111</td>\n",
       "      <td>1.001386e+01</td>\n",
       "      <td>1.001371e+01</td>\n",
       "      <td>10.013720</td>\n",
       "      <td>10.013862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.003076</td>\n",
       "      <td>10.000057</td>\n",
       "      <td>9.998053</td>\n",
       "      <td>9.997400</td>\n",
       "      <td>9.998204</td>\n",
       "      <td>10.000222</td>\n",
       "      <td>1.000285e+01</td>\n",
       "      <td>10.005251</td>\n",
       "      <td>10.006590</td>\n",
       "      <td>10.006328</td>\n",
       "      <td>...</td>\n",
       "      <td>10.007767</td>\n",
       "      <td>10.010679</td>\n",
       "      <td>10.014165</td>\n",
       "      <td>10.017877</td>\n",
       "      <td>10.021456</td>\n",
       "      <td>10.024579</td>\n",
       "      <td>1.002701e+01</td>\n",
       "      <td>1.002862e+01</td>\n",
       "      <td>10.029415</td>\n",
       "      <td>10.029448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4.811054e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.557120e-07</td>\n",
       "      <td>9.344143e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.882745</td>\n",
       "      <td>2.882236</td>\n",
       "      <td>2.887322</td>\n",
       "      <td>2.886715</td>\n",
       "      <td>2.878279</td>\n",
       "      <td>2.879955</td>\n",
       "      <td>2.881660e+00</td>\n",
       "      <td>2.882988</td>\n",
       "      <td>2.882563</td>\n",
       "      <td>2.880368</td>\n",
       "      <td>...</td>\n",
       "      <td>2.882248</td>\n",
       "      <td>2.883979</td>\n",
       "      <td>2.880040</td>\n",
       "      <td>2.885831</td>\n",
       "      <td>2.880396</td>\n",
       "      <td>2.880352</td>\n",
       "      <td>2.877562e+00</td>\n",
       "      <td>2.878891e+00</td>\n",
       "      <td>2.882343</td>\n",
       "      <td>2.885722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.934941</td>\n",
       "      <td>6.933017</td>\n",
       "      <td>6.939023</td>\n",
       "      <td>6.939193</td>\n",
       "      <td>6.940237</td>\n",
       "      <td>6.943029</td>\n",
       "      <td>6.938943e+00</td>\n",
       "      <td>6.935458</td>\n",
       "      <td>6.936254</td>\n",
       "      <td>6.932065</td>\n",
       "      <td>...</td>\n",
       "      <td>6.941200</td>\n",
       "      <td>6.946095</td>\n",
       "      <td>6.945471</td>\n",
       "      <td>6.947915</td>\n",
       "      <td>6.939139</td>\n",
       "      <td>6.944599</td>\n",
       "      <td>6.940263e+00</td>\n",
       "      <td>6.944825e+00</td>\n",
       "      <td>6.934469</td>\n",
       "      <td>6.939888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.843788</td>\n",
       "      <td>13.865951</td>\n",
       "      <td>13.875038</td>\n",
       "      <td>13.882409</td>\n",
       "      <td>13.886576</td>\n",
       "      <td>13.884771</td>\n",
       "      <td>1.388077e+01</td>\n",
       "      <td>13.880488</td>\n",
       "      <td>13.868526</td>\n",
       "      <td>13.881989</td>\n",
       "      <td>...</td>\n",
       "      <td>13.885061</td>\n",
       "      <td>13.875917</td>\n",
       "      <td>13.871919</td>\n",
       "      <td>13.878928</td>\n",
       "      <td>13.889511</td>\n",
       "      <td>13.880936</td>\n",
       "      <td>1.387604e+01</td>\n",
       "      <td>1.387670e+01</td>\n",
       "      <td>13.871021</td>\n",
       "      <td>13.870399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>149.928490</td>\n",
       "      <td>158.053010</td>\n",
       "      <td>160.410605</td>\n",
       "      <td>156.827589</td>\n",
       "      <td>147.622207</td>\n",
       "      <td>133.570093</td>\n",
       "      <td>1.294800e+02</td>\n",
       "      <td>131.301800</td>\n",
       "      <td>140.857127</td>\n",
       "      <td>156.519784</td>\n",
       "      <td>...</td>\n",
       "      <td>148.344790</td>\n",
       "      <td>156.795340</td>\n",
       "      <td>163.599276</td>\n",
       "      <td>164.152879</td>\n",
       "      <td>165.848467</td>\n",
       "      <td>162.582232</td>\n",
       "      <td>1.547072e+02</td>\n",
       "      <td>1.504642e+02</td>\n",
       "      <td>140.904372</td>\n",
       "      <td>137.098610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0               1               2               3   \\\n",
       "count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   \n",
       "mean        10.003086       10.004461       10.005724       10.006757   \n",
       "std         10.003076       10.000057        9.998053        9.997400   \n",
       "min          0.000006        0.000033        0.000021        0.000023   \n",
       "25%          2.882745        2.882236        2.887322        2.886715   \n",
       "50%          6.934941        6.933017        6.939023        6.939193   \n",
       "75%         13.843788       13.865951       13.875038       13.882409   \n",
       "max        149.928490      158.053010      160.410605      156.827589   \n",
       "\n",
       "                   4               5             6               7   \\\n",
       "count  1000000.000000  1000000.000000  1.000000e+06  1000000.000000   \n",
       "mean        10.007477       10.007848  1.000789e+01       10.007660   \n",
       "std          9.998204       10.000222  1.000285e+01       10.005251   \n",
       "min          0.000006        0.000004  4.811054e-07        0.000002   \n",
       "25%          2.878279        2.879955  2.881660e+00        2.882988   \n",
       "50%          6.940237        6.943029  6.938943e+00        6.935458   \n",
       "75%         13.886576       13.884771  1.388077e+01       13.880488   \n",
       "max        147.622207      133.570093  1.294800e+02      131.301800   \n",
       "\n",
       "                   8               9   ...              90              91  \\\n",
       "count  1000000.000000  1000000.000000  ...  1000000.000000  1000000.000000   \n",
       "mean        10.007274       10.006859  ...       10.013202       10.014060   \n",
       "std         10.006590       10.006328  ...       10.007767       10.010679   \n",
       "min          0.000002        0.000004  ...        0.000009        0.000014   \n",
       "25%          2.882563        2.880368  ...        2.882248        2.883979   \n",
       "50%          6.936254        6.932065  ...        6.941200        6.946095   \n",
       "75%         13.868526       13.881989  ...       13.885061       13.875917   \n",
       "max        140.857127      156.519784  ...      148.344790      156.795340   \n",
       "\n",
       "                   92              93              94              95  \\\n",
       "count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   \n",
       "mean        10.014483       10.014552       10.014383       10.014111   \n",
       "std         10.014165       10.017877       10.021456       10.024579   \n",
       "min          0.000007        0.000004        0.000005        0.000004   \n",
       "25%          2.880040        2.885831        2.880396        2.880352   \n",
       "50%          6.945471        6.947915        6.939139        6.944599   \n",
       "75%         13.871919       13.878928       13.889511       13.880936   \n",
       "max        163.599276      164.152879      165.848467      162.582232   \n",
       "\n",
       "                 96            97              98              99  \n",
       "count  1.000000e+06  1.000000e+06  1000000.000000  1000000.000000  \n",
       "mean   1.001386e+01  1.001371e+01       10.013720       10.013862  \n",
       "std    1.002701e+01  1.002862e+01       10.029415       10.029448  \n",
       "min    9.557120e-07  9.344143e-07        0.000004        0.000002  \n",
       "25%    2.877562e+00  2.878891e+00        2.882343        2.885722  \n",
       "50%    6.940263e+00  6.944825e+00        6.934469        6.939888  \n",
       "75%    1.387604e+01  1.387670e+01       13.871021       13.870399  \n",
       "max    1.547072e+02  1.504642e+02      140.904372      137.098610  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dados.size)\n",
    "print(dados.shape)\n",
    "dados.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>24</th>\n",
       "      <th>49</th>\n",
       "      <th>74</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.003086</td>\n",
       "      <td>10.011016</td>\n",
       "      <td>9.999887</td>\n",
       "      <td>10.018376</td>\n",
       "      <td>10.013862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.003076</td>\n",
       "      <td>10.022521</td>\n",
       "      <td>9.987231</td>\n",
       "      <td>10.013229</td>\n",
       "      <td>10.029448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.882745</td>\n",
       "      <td>2.879628</td>\n",
       "      <td>2.875682</td>\n",
       "      <td>2.880966</td>\n",
       "      <td>2.885722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.934941</td>\n",
       "      <td>6.944056</td>\n",
       "      <td>6.932545</td>\n",
       "      <td>6.933767</td>\n",
       "      <td>6.939888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.843788</td>\n",
       "      <td>13.869075</td>\n",
       "      <td>13.874761</td>\n",
       "      <td>13.886951</td>\n",
       "      <td>13.870399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>149.928490</td>\n",
       "      <td>159.658812</td>\n",
       "      <td>135.762065</td>\n",
       "      <td>175.306850</td>\n",
       "      <td>137.098610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0               24              49              74  \\\n",
       "count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   \n",
       "mean        10.003086       10.011016        9.999887       10.018376   \n",
       "std         10.003076       10.022521        9.987231       10.013229   \n",
       "min          0.000006        0.000019        0.000011        0.000008   \n",
       "25%          2.882745        2.879628        2.875682        2.880966   \n",
       "50%          6.934941        6.944056        6.932545        6.933767   \n",
       "75%         13.843788       13.869075       13.874761       13.886951   \n",
       "max        149.928490      159.658812      135.762065      175.306850   \n",
       "\n",
       "                   99  \n",
       "count  1000000.000000  \n",
       "mean        10.013862  \n",
       "std         10.029448  \n",
       "min          0.000002  \n",
       "25%          2.885722  \n",
       "50%          6.939888  \n",
       "75%         13.870399  \n",
       "max        137.098610  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduzir_array(arr, novo_tamanho):\n",
    "    indices = np.linspace(0, arr.shape[1]-1, novo_tamanho, dtype=int)\n",
    "    array_reduzido = arr[indices]\n",
    "    return array_reduzido, indices\n",
    "\n",
    "dados_, idx = reduzir_array(dados, number_of_ports)\n",
    "dados_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dados_.iloc[:, :].to_numpy()\n",
    "Xindices = idx\n",
    "y = dados.iloc[:, :].to_numpy()                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dados_\n",
    "del idx\n",
    "del dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1000000\n",
      "Train 800000\n",
      "Val 200000\n"
     ]
    }
   ],
   "source": [
    "# Split the whole set into random training and validation set.\n",
    "X_train_, X_val_, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "print('Total:', len(y))\n",
    "print('Train', len(y_train))\n",
    "print('Val', len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xindices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_lstm_fama_sinr_regression_5_ports_15102024b\n"
     ]
    }
   ],
   "source": [
    "version = '15102024b'\n",
    "project_name = 'cnn_lstm_fama_sinr_regression_'+str(number_of_ports)+'_ports_'+version\n",
    "print(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referência \n",
    "# https://github.com/ashrefm/multi-label-soft-f1/blob/master/Multi-Label%20Image%20Classification%20in%20TensorFlow%202.0.ipynb\n",
    "\n",
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOp(Xindices, y_val_pred, y_val):\n",
    "    '''Estimate outage probability for regression models'''\n",
    "    \n",
    "    # Após a predição do modelo, verificar se a escolha foi melhor que a referência (i.e., melhor que as N portas observadas)\n",
    "    # Caso contrário, se uma porta observada tiver sido melhor que a predição, escolhe a porta observada.\n",
    "    \n",
    "    # Monta um array com o valor da porta observada\n",
    "    observed_values = np.zeros((y_val.shape[0], 100)) - np.float64('inf')\n",
    "    observed_values[:, Xindices] = y_val[:, Xindices]\n",
    "\n",
    "    pred_values = np.argmax(y_val_pred, axis=1)\n",
    "\n",
    "    predicted_values = np.zeros((y_val.shape[0], 100)) - np.float64('inf')\n",
    "    predicted_values[np.arange(len(pred_values)), pred_values] = y_val[np.arange(len(pred_values)), pred_values]\n",
    "    \n",
    "    # Pega o máximo entre as portas observadas e preditadas\n",
    "    maximo_entre_arrays = np.maximum(observed_values, predicted_values)\n",
    "\n",
    "    print(\"Max Arrays:\", maximo_entre_arrays.shape)\n",
    "    \n",
    "    # Indice (Porta) da melhor predição/observada\n",
    "    best_pred_obs_port = np.argmax(maximo_entre_arrays, axis=1)\n",
    "\n",
    "    print(\"Best Pred Obs:\", best_pred_obs_port.shape)\n",
    "\n",
    "    print(\"Best_Ports_Len = \", len(best_pred_obs_port))\n",
    "    \n",
    "    # Valor do dado real na porta escolhida pela predição\n",
    "    values_for_predicted_classes = maximo_entre_arrays[np.arange(len(y_val)), best_pred_obs_port]\n",
    "\n",
    "    print(\"Values Preds:\", values_for_predicted_classes.shape)\n",
    "    \n",
    "    # True or False para os valores que ficaram acima do limiar do gamma_th\n",
    "    # acima_do_limiar = values_for_predicted_classes > gamma_th_linear\n",
    "    # acima_do_limiar = values_for_predicted_classes > gamma_th\n",
    "    acima_do_limiar = values_for_predicted_classes > threshold\n",
    "\n",
    "    print(\"Above thresh:\", acima_do_limiar.shape)\n",
    "    \n",
    "    # Probabilidade de outage\n",
    "    p_out = 1.0 - (np.sum(acima_do_limiar) / len(y_val))\n",
    "    \n",
    "    return p_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 1\n",
    "n_features = number_of_ports\n",
    "n_seq = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    global X_train_\n",
    "    global X_val_\n",
    "    \n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    clear_session()    \n",
    "\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'linear', 'sigmoid', 'tanh'])\n",
    "    cnn = trial.suggest_categorical('cnn', [0, 1, 2])\n",
    "    filters = trial.suggest_int('filters', 1, 512)\n",
    "    kernel_size = trial.suggest_categorical('kernel_size', [1, 2, 3, 4, 5])\n",
    "    learning_rate = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    dense = trial.suggest_categorical('dense', [0, 1, 2, 3])\n",
    "    lstm = trial.suggest_categorical('lstm', [1, 2, 3])\n",
    "    \n",
    "    pool = trial.suggest_categorical('pool', ['max', 'avg'])\n",
    "    if(pool == 'max'):\n",
    "        pool_layer = MaxPooling1D(pool_size=1)\n",
    "    else:\n",
    "        pool_layer = AveragePooling1D(pool_size=1)\n",
    "\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd', 'nadam'])\n",
    "    if optimizer=='adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer=='nadam':\n",
    "        opt = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "    elif optimizer=='sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    scaler = trial.suggest_categorical('scaler', ['std', 'minmax', 'none'])\n",
    "    if(scaler == 'std'):\n",
    "        scl = StandardScaler()\n",
    "    else:\n",
    "        scl = MinMaxScaler(feature_range=(-1, 1))\n",
    "    \n",
    "    if(scaler=='std' or scaler=='minmax'):\n",
    "        X_train__ = scl.fit_transform(X_train_)\n",
    "        X_val__ = scl.transform(X_val_)\n",
    "    else:\n",
    "        X_train__ = X_train_\n",
    "        X_val__ = X_val_\n",
    "    \n",
    "    pca = trial.suggest_categorical('pca', ['yes', 'no'])\n",
    "    if(pca == 'yes'):\n",
    "        pCA = PCA()\n",
    "        X_train___ = pCA.fit_transform(X_train__)\n",
    "        X_val___ = pCA.transform(X_val__)\n",
    "    else:\n",
    "        X_train___ = X_train__\n",
    "        X_val___ = X_val__\n",
    "    \n",
    "    \n",
    "    del X_train__\n",
    "    del X_val__\n",
    "\n",
    "    if(cnn>0):\n",
    "        X_train = X_train___.reshape((X_train___.shape[0], n_seq, n_steps, n_features))\n",
    "        X_val = X_val___.reshape((X_val___.shape[0], n_seq, n_steps, n_features))\n",
    "    else:\n",
    "        X_train = X_train___.reshape((X_train___.shape[0], n_steps, n_features))\n",
    "        X_val = X_val___.reshape((X_val___.shape[0], n_steps, n_features))\n",
    "\n",
    "    del X_train___\n",
    "    del X_val___\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    \n",
    "    if(cnn>0):\n",
    "        sp=(None, n_steps, n_features)\n",
    "    else:\n",
    "        sp=(n_steps, n_features)\n",
    "    model.add(Input(shape=sp))\n",
    "    \n",
    "    if(cnn>0):\n",
    "\n",
    "        for i in range(cnn):\n",
    "            model.add(TimeDistributed(Conv1D(filters=filters, kernel_size=kernel_size, padding='same', activation='relu')))\n",
    "            model.add(TimeDistributed(pool_layer))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    for i in range(lstm):\n",
    "        cells = trial.suggest_int('cells_'+str(i), 1, 100)\n",
    "        if(lstm > 1 and i < lstm-1):\n",
    "            return_sequences=True\n",
    "        else:\n",
    "            return_sequences=False\n",
    "        model.add(LSTM(cells, activation=activation, return_sequences=return_sequences))\n",
    "    for i in range(dense):\n",
    "        nodes = trial.suggest_int('nodes_'+str(i), 1, 300)\n",
    "        model.add(Dense(nodes, activation='relu'))\n",
    "        dropout = trial.suggest_categorical('dropout_'+str(i), [0.0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(100, activation='linear'))\n",
    "\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=opt,\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    es_cb = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)    \n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=2048, epochs=50, verbose=True, callbacks=[es_cb])\n",
    "    \n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    #score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    y_val_pred = model.predict(X_val, batch_size=2048)\n",
    "    \n",
    "    op = getOp(Xindices, y_val_pred, y_val)\n",
    "    \n",
    "    return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 14:13:19,917] Using an existing study with name 'study_cnn_lstm_fama_sinr_regression_5_ports_15102024b' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampler is TPESampler\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "391/391 [==============================] - 7s 12ms/step - loss: 115.1329 - mae: 7.4537 - val_loss: 73.9450 - val_mae: 6.0373\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 78.1978 - mae: 6.2249 - val_loss: 66.4206 - val_mae: 5.5684\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 73.5534 - mae: 6.0001 - val_loss: 64.3372 - val_mae: 5.4660\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 71.6655 - mae: 5.9049 - val_loss: 63.6275 - val_mae: 5.3934\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 70.7560 - mae: 5.8547 - val_loss: 63.1096 - val_mae: 5.3916\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 70.2337 - mae: 5.8241 - val_loss: 63.0272 - val_mae: 5.3616\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 69.8659 - mae: 5.8015 - val_loss: 62.9562 - val_mae: 5.3562\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 69.5885 - mae: 5.7858 - val_loss: 62.7885 - val_mae: 5.3557\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 69.3945 - mae: 5.7725 - val_loss: 62.9131 - val_mae: 5.3378\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 69.2410 - mae: 5.7630 - val_loss: 62.6583 - val_mae: 5.3529\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 69.0794 - mae: 5.7534 - val_loss: 62.8311 - val_mae: 5.3287\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 68.9808 - mae: 5.7466 - val_loss: 62.7259 - val_mae: 5.3397\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 68.8810 - mae: 5.7407 - val_loss: 62.6736 - val_mae: 5.3350\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 68.7699 - mae: 5.7339 - val_loss: 62.5501 - val_mae: 5.3471\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 68.6728 - mae: 5.7294 - val_loss: 62.4998 - val_mae: 5.3322\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 68.6045 - mae: 5.7248 - val_loss: 62.5249 - val_mae: 5.3283\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 68.4982 - mae: 5.7192 - val_loss: 62.5665 - val_mae: 5.3356\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 68.4157 - mae: 5.7143 - val_loss: 62.4675 - val_mae: 5.3215\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 68.3295 - mae: 5.7103 - val_loss: 62.4304 - val_mae: 5.3407\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 68.2330 - mae: 5.7057 - val_loss: 62.4508 - val_mae: 5.3400\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 68.1497 - mae: 5.7019 - val_loss: 62.4182 - val_mae: 5.3474\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 68.0497 - mae: 5.6972 - val_loss: 62.4827 - val_mae: 5.3215\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.9878 - mae: 5.6934 - val_loss: 62.4266 - val_mae: 5.3271\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.8677 - mae: 5.6892 - val_loss: 62.4079 - val_mae: 5.3261\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 67.7713 - mae: 5.6846 - val_loss: 62.4384 - val_mae: 5.3206\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.7106 - mae: 5.6815 - val_loss: 62.3712 - val_mae: 5.3263\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.6531 - mae: 5.6786 - val_loss: 62.3484 - val_mae: 5.3291\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.5887 - mae: 5.6756 - val_loss: 62.3033 - val_mae: 5.3297\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 67.5083 - mae: 5.6718 - val_loss: 62.3420 - val_mae: 5.3266\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.4348 - mae: 5.6694 - val_loss: 62.2805 - val_mae: 5.3359\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.3723 - mae: 5.6670 - val_loss: 62.3789 - val_mae: 5.3392\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.2973 - mae: 5.6635 - val_loss: 62.2446 - val_mae: 5.3392\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.2322 - mae: 5.6620 - val_loss: 62.2561 - val_mae: 5.3394\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.1780 - mae: 5.6603 - val_loss: 62.2454 - val_mae: 5.3331\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.1304 - mae: 5.6580 - val_loss: 62.2959 - val_mae: 5.3378\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 67.0719 - mae: 5.6557 - val_loss: 62.2807 - val_mae: 5.3368\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.0321 - mae: 5.6549 - val_loss: 62.2347 - val_mae: 5.3381\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 67.0077 - mae: 5.6536 - val_loss: 62.2169 - val_mae: 5.3336\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 66.9484 - mae: 5.6513 - val_loss: 62.2927 - val_mae: 5.3327\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 66.9092 - mae: 5.6492 - val_loss: 62.1722 - val_mae: 5.3360\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 66.8930 - mae: 5.6488 - val_loss: 62.1692 - val_mae: 5.3407\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 66.8533 - mae: 5.6475 - val_loss: 62.1693 - val_mae: 5.3380\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 66.8153 - mae: 5.6459 - val_loss: 62.3055 - val_mae: 5.3375\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 66.7955 - mae: 5.6441 - val_loss: 62.3400 - val_mae: 5.3424\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 66.7492 - mae: 5.6423 - val_loss: 62.2058 - val_mae: 5.3409\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 66.7171 - mae: 5.6406 - val_loss: 62.1717 - val_mae: 5.3362\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 66.7052 - mae: 5.6399 - val_loss: 62.1213 - val_mae: 5.3424\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 66.6835 - mae: 5.6388 - val_loss: 62.1885 - val_mae: 5.3376\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 66.6612 - mae: 5.6371 - val_loss: 62.1342 - val_mae: 5.3338\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 66.6324 - mae: 5.6359 - val_loss: 62.1674 - val_mae: 5.3446\n",
      "98/98 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 14:16:51,399] Trial 1 finished with value: 0.08098000000000005 and parameters: {'activation': 'linear', 'cnn': 0, 'filters': 237, 'kernel_size': 2, 'lr': 0.0002605947799523198, 'dense': 2, 'lstm': 2, 'pool': 'max', 'optimizer': 'adam', 'scaler': 'none', 'pca': 'no', 'cells_0': 46, 'cells_1': 61, 'nodes_0': 188, 'dropout_0': 0.5, 'nodes_1': 30, 'dropout_1': 0.2}. Best is trial 1 with value: 0.08098000000000005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Arrays: (200000, 100)\n",
      "Best Pred Obs: (200000,)\n",
      "Best_Ports_Len =  200000\n",
      "Values Preds: (200000,)\n",
      "Above thresh: (200000,)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "391/391 [==============================] - 10s 17ms/step - loss: 209.4414 - mae: 8.4994 - val_loss: 121.2580 - val_mae: 7.0604\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 108.9190 - mae: 6.9977 - val_loss: 101.2610 - val_mae: 7.0750\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.8555 - mae: 7.2215 - val_loss: 99.6350 - val_mae: 7.2980\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3533 - mae: 7.3455 - val_loss: 99.5956 - val_mae: 7.3478\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3420 - mae: 7.3661 - val_loss: 99.5959 - val_mae: 7.3526\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3421 - mae: 7.3685 - val_loss: 99.5960 - val_mae: 7.3514\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3421 - mae: 7.3679 - val_loss: 99.5958 - val_mae: 7.3508\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3422 - mae: 7.3672 - val_loss: 99.5964 - val_mae: 7.3545\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3422 - mae: 7.3681 - val_loss: 99.5965 - val_mae: 7.3514\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3424 - mae: 7.3677 - val_loss: 99.5960 - val_mae: 7.3521\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3425 - mae: 7.3676 - val_loss: 99.5968 - val_mae: 7.3540\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3425 - mae: 7.3675 - val_loss: 99.5974 - val_mae: 7.3559\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3428 - mae: 7.3683 - val_loss: 99.5971 - val_mae: 7.3529\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3430 - mae: 7.3682 - val_loss: 99.5968 - val_mae: 7.3471\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3431 - mae: 7.3676 - val_loss: 99.5969 - val_mae: 7.3491\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3436 - mae: 7.3679 - val_loss: 99.5973 - val_mae: 7.3547\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3435 - mae: 7.3677 - val_loss: 99.6003 - val_mae: 7.3549\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3433 - mae: 7.3678 - val_loss: 99.5978 - val_mae: 7.3506\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3437 - mae: 7.3679 - val_loss: 99.5975 - val_mae: 7.3557\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3437 - mae: 7.3678 - val_loss: 99.5990 - val_mae: 7.3575\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3439 - mae: 7.3684 - val_loss: 99.5962 - val_mae: 7.3460\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3442 - mae: 7.3679 - val_loss: 99.5976 - val_mae: 7.3533\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3441 - mae: 7.3674 - val_loss: 99.5976 - val_mae: 7.3552\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3440 - mae: 7.3678 - val_loss: 99.5984 - val_mae: 7.3543\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3439 - mae: 7.3682 - val_loss: 99.5980 - val_mae: 7.3469\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3445 - mae: 7.3676 - val_loss: 99.5992 - val_mae: 7.3589\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3438 - mae: 7.3680 - val_loss: 99.5985 - val_mae: 7.3528\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3442 - mae: 7.3681 - val_loss: 99.5955 - val_mae: 7.3457\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3440 - mae: 7.3676 - val_loss: 99.5985 - val_mae: 7.3537\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3439 - mae: 7.3677 - val_loss: 99.5972 - val_mae: 7.3518\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3443 - mae: 7.3682 - val_loss: 99.5979 - val_mae: 7.3464\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3441 - mae: 7.3680 - val_loss: 99.5971 - val_mae: 7.3467\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3441 - mae: 7.3676 - val_loss: 99.5969 - val_mae: 7.3497\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3439 - mae: 7.3681 - val_loss: 99.5982 - val_mae: 7.3412\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3442 - mae: 7.3678 - val_loss: 99.5960 - val_mae: 7.3483\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3445 - mae: 7.3676 - val_loss: 99.6021 - val_mae: 7.3611\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3446 - mae: 7.3680 - val_loss: 99.5972 - val_mae: 7.3508\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3445 - mae: 7.3681 - val_loss: 99.5977 - val_mae: 7.3529\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3442 - mae: 7.3680 - val_loss: 99.6007 - val_mae: 7.3537\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3442 - mae: 7.3678 - val_loss: 99.5970 - val_mae: 7.3492\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3443 - mae: 7.3681 - val_loss: 99.5970 - val_mae: 7.3510\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3442 - mae: 7.3679 - val_loss: 99.5976 - val_mae: 7.3531\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3449 - mae: 7.3676 - val_loss: 99.5984 - val_mae: 7.3545\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3438 - mae: 7.3682 - val_loss: 99.5979 - val_mae: 7.3453\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3447 - mae: 7.3676 - val_loss: 99.5980 - val_mae: 7.3518\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3445 - mae: 7.3683 - val_loss: 99.5991 - val_mae: 7.3441\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3445 - mae: 7.3677 - val_loss: 99.5968 - val_mae: 7.3475\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3444 - mae: 7.3681 - val_loss: 99.5969 - val_mae: 7.3469\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3442 - mae: 7.3675 - val_loss: 99.5991 - val_mae: 7.3540\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.3447 - mae: 7.3680 - val_loss: 99.5969 - val_mae: 7.3483\n",
      "98/98 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 14:22:11,422] Trial 2 finished with value: 0.11848999999999998 and parameters: {'activation': 'linear', 'cnn': 2, 'filters': 239, 'kernel_size': 5, 'lr': 0.022958779280784113, 'dense': 1, 'lstm': 3, 'pool': 'avg', 'optimizer': 'adam', 'scaler': 'std', 'pca': 'yes', 'cells_0': 62, 'cells_1': 43, 'cells_2': 80, 'nodes_0': 166, 'dropout_0': 0.1}. Best is trial 1 with value: 0.08098000000000005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Arrays: (200000, 100)\n",
      "Best Pred Obs: (200000,)\n",
      "Best_Ports_Len =  200000\n",
      "Values Preds: (200000,)\n",
      "Above thresh: (200000,)\n",
      "Epoch 1/50\n",
      "391/391 [==============================] - 6s 9ms/step - loss: 155.4180 - mae: 8.3372 - val_loss: 97.9967 - val_mae: 6.8215\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 93.9850 - mae: 6.9844 - val_loss: 92.2869 - val_mae: 7.0154\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 92.7202 - mae: 7.0267 - val_loss: 91.8905 - val_mae: 7.0057\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 92.3914 - mae: 7.0158 - val_loss: 91.6106 - val_mae: 6.9964\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 92.1206 - mae: 7.0074 - val_loss: 91.3523 - val_mae: 6.9944\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 91.8392 - mae: 6.9990 - val_loss: 91.0412 - val_mae: 6.9794\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 91.3820 - mae: 6.9804 - val_loss: 90.3126 - val_mae: 6.9447\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 89.6736 - mae: 6.8923 - val_loss: 87.1668 - val_mae: 6.7787\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 85.8259 - mae: 6.7048 - val_loss: 83.6424 - val_mae: 6.6183\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 83.1503 - mae: 6.5878 - val_loss: 81.6217 - val_mae: 6.5332\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 81.3250 - mae: 6.5020 - val_loss: 79.7026 - val_mae: 6.4390\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 78.9451 - mae: 6.3849 - val_loss: 77.0204 - val_mae: 6.3114\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 76.6490 - mae: 6.2847 - val_loss: 75.2035 - val_mae: 6.2332\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 75.0398 - mae: 6.2049 - val_loss: 73.7484 - val_mae: 6.1471\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 73.8052 - mae: 6.1192 - val_loss: 72.7578 - val_mae: 6.0647\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 72.9928 - mae: 6.0617 - val_loss: 72.0763 - val_mae: 6.0305\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 72.3627 - mae: 6.0308 - val_loss: 71.4914 - val_mae: 6.0044\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 71.8114 - mae: 6.0062 - val_loss: 70.9869 - val_mae: 5.9741\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 71.3334 - mae: 5.9807 - val_loss: 70.5548 - val_mae: 5.9482\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 70.9080 - mae: 5.9556 - val_loss: 70.1535 - val_mae: 5.9376\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 70.4950 - mae: 5.9321 - val_loss: 69.7413 - val_mae: 5.9104\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 70.0946 - mae: 5.9100 - val_loss: 69.3675 - val_mae: 5.8867\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 69.7510 - mae: 5.8929 - val_loss: 69.0561 - val_mae: 5.8730\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 69.4608 - mae: 5.8805 - val_loss: 68.7793 - val_mae: 5.8571\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 69.2031 - mae: 5.8709 - val_loss: 68.5340 - val_mae: 5.8548\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 68.9644 - mae: 5.8627 - val_loss: 68.3006 - val_mae: 5.8407\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 68.7355 - mae: 5.8532 - val_loss: 68.0763 - val_mae: 5.8338\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 68.5096 - mae: 5.8436 - val_loss: 67.8498 - val_mae: 5.8242\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 68.2858 - mae: 5.8326 - val_loss: 67.6354 - val_mae: 5.8147\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 68.0737 - mae: 5.8218 - val_loss: 67.4273 - val_mae: 5.7995\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 67.8744 - mae: 5.8101 - val_loss: 67.2315 - val_mae: 5.7894\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 67.6930 - mae: 5.7990 - val_loss: 67.0536 - val_mae: 5.7818\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 67.5378 - mae: 5.7893 - val_loss: 66.9069 - val_mae: 5.7739\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 67.4069 - mae: 5.7801 - val_loss: 66.7852 - val_mae: 5.7623\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 67.2965 - mae: 5.7724 - val_loss: 66.6725 - val_mae: 5.7523\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 67.2020 - mae: 5.7653 - val_loss: 66.5858 - val_mae: 5.7461\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 67.1212 - mae: 5.7594 - val_loss: 66.5074 - val_mae: 5.7394\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 67.0504 - mae: 5.7540 - val_loss: 66.4382 - val_mae: 5.7363\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.9879 - mae: 5.7494 - val_loss: 66.3798 - val_mae: 5.7357\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.9320 - mae: 5.7453 - val_loss: 66.3317 - val_mae: 5.7293\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.8818 - mae: 5.7416 - val_loss: 66.2888 - val_mae: 5.7232\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.8366 - mae: 5.7381 - val_loss: 66.2480 - val_mae: 5.7212\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.7958 - mae: 5.7352 - val_loss: 66.2128 - val_mae: 5.7182\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.7588 - mae: 5.7324 - val_loss: 66.1857 - val_mae: 5.7270\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.7247 - mae: 5.7301 - val_loss: 66.1506 - val_mae: 5.7122\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.6930 - mae: 5.7275 - val_loss: 66.1267 - val_mae: 5.7177\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.6651 - mae: 5.7257 - val_loss: 66.0968 - val_mae: 5.7040\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.6388 - mae: 5.7237 - val_loss: 66.0735 - val_mae: 5.7025\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.6143 - mae: 5.7218 - val_loss: 66.0531 - val_mae: 5.7087\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 66.5929 - mae: 5.7202 - val_loss: 66.0286 - val_mae: 5.7069\n",
      "98/98 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 14:24:29,480] Trial 3 finished with value: 0.07949499999999998 and parameters: {'activation': 'tanh', 'cnn': 0, 'filters': 469, 'kernel_size': 4, 'lr': 0.0011624014200319027, 'dense': 0, 'lstm': 2, 'pool': 'avg', 'optimizer': 'adam', 'scaler': 'std', 'pca': 'yes', 'cells_0': 3, 'cells_1': 28}. Best is trial 3 with value: 0.07949499999999998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Arrays: (200000, 100)\n",
      "Best Pred Obs: (200000,)\n",
      "Best_Ports_Len =  200000\n",
      "Values Preds: (200000,)\n",
      "Above thresh: (200000,)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "391/391 [==============================] - 9s 16ms/step - loss: 81.7410 - mae: 6.2855 - val_loss: 63.1351 - val_mae: 5.5143\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 65.2705 - mae: 5.5606 - val_loss: 62.2395 - val_mae: 5.3040\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 64.3652 - mae: 5.4989 - val_loss: 61.8612 - val_mae: 5.3041\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 64.0230 - mae: 5.4744 - val_loss: 61.7549 - val_mae: 5.3406\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.7833 - mae: 5.4573 - val_loss: 61.7660 - val_mae: 5.2818\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.6412 - mae: 5.4469 - val_loss: 61.6514 - val_mae: 5.3176\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.5596 - mae: 5.4401 - val_loss: 61.7081 - val_mae: 5.2737\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.4710 - mae: 5.4336 - val_loss: 61.6553 - val_mae: 5.2672\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.4264 - mae: 5.4294 - val_loss: 61.6126 - val_mae: 5.3189\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.3882 - mae: 5.4260 - val_loss: 61.6093 - val_mae: 5.2861\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.3438 - mae: 5.4224 - val_loss: 61.8941 - val_mae: 5.2445\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.3140 - mae: 5.4197 - val_loss: 61.6012 - val_mae: 5.3053\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.2889 - mae: 5.4177 - val_loss: 61.6302 - val_mae: 5.2690\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.2726 - mae: 5.4161 - val_loss: 61.5870 - val_mae: 5.2783\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.2300 - mae: 5.4134 - val_loss: 61.5741 - val_mae: 5.3085\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.2214 - mae: 5.4120 - val_loss: 61.5826 - val_mae: 5.3131\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.2053 - mae: 5.4112 - val_loss: 61.5807 - val_mae: 5.2844\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.1904 - mae: 5.4094 - val_loss: 61.7533 - val_mae: 5.2595\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.1688 - mae: 5.4082 - val_loss: 61.6161 - val_mae: 5.2743\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.1550 - mae: 5.4070 - val_loss: 61.5724 - val_mae: 5.3015\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.1410 - mae: 5.4060 - val_loss: 61.5788 - val_mae: 5.2988\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.1255 - mae: 5.4046 - val_loss: 61.5633 - val_mae: 5.3016\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.1087 - mae: 5.4037 - val_loss: 61.5841 - val_mae: 5.2812\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.1027 - mae: 5.4029 - val_loss: 61.5876 - val_mae: 5.2766\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0930 - mae: 5.4021 - val_loss: 61.5618 - val_mae: 5.2895\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0710 - mae: 5.4007 - val_loss: 61.5559 - val_mae: 5.3049\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0663 - mae: 5.4009 - val_loss: 61.6231 - val_mae: 5.2705\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0597 - mae: 5.3994 - val_loss: 61.5921 - val_mae: 5.2803\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0476 - mae: 5.3991 - val_loss: 61.5732 - val_mae: 5.2898\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0452 - mae: 5.3986 - val_loss: 61.6083 - val_mae: 5.2676\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0428 - mae: 5.3981 - val_loss: 61.5673 - val_mae: 5.2852\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0274 - mae: 5.3975 - val_loss: 61.5714 - val_mae: 5.2868\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0131 - mae: 5.3968 - val_loss: 61.5757 - val_mae: 5.2930\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0191 - mae: 5.3965 - val_loss: 61.5931 - val_mae: 5.2679\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0093 - mae: 5.3957 - val_loss: 61.5677 - val_mae: 5.2951\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0039 - mae: 5.3955 - val_loss: 61.5713 - val_mae: 5.3098\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.0026 - mae: 5.3953 - val_loss: 61.5647 - val_mae: 5.2924\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9868 - mae: 5.3945 - val_loss: 61.5565 - val_mae: 5.3015\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9746 - mae: 5.3937 - val_loss: 61.5687 - val_mae: 5.3050\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9781 - mae: 5.3935 - val_loss: 61.5745 - val_mae: 5.3077\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9687 - mae: 5.3932 - val_loss: 61.5619 - val_mae: 5.2868\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9650 - mae: 5.3928 - val_loss: 61.5643 - val_mae: 5.2767\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9501 - mae: 5.3918 - val_loss: 61.5845 - val_mae: 5.2816\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9549 - mae: 5.3921 - val_loss: 61.5980 - val_mae: 5.2873\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9498 - mae: 5.3915 - val_loss: 61.5932 - val_mae: 5.2842\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9470 - mae: 5.3914 - val_loss: 61.5801 - val_mae: 5.2919\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9318 - mae: 5.3907 - val_loss: 61.5885 - val_mae: 5.2812\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9277 - mae: 5.3905 - val_loss: 61.5652 - val_mae: 5.2815\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9212 - mae: 5.3898 - val_loss: 61.5879 - val_mae: 5.2774\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9181 - mae: 5.3893 - val_loss: 61.5582 - val_mae: 5.2967\n",
      "98/98 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 14:29:27,629] Trial 4 finished with value: 0.07865999999999995 and parameters: {'activation': 'relu', 'cnn': 0, 'filters': 139, 'kernel_size': 5, 'lr': 0.0006229115317675619, 'dense': 3, 'lstm': 2, 'pool': 'max', 'optimizer': 'nadam', 'scaler': 'none', 'pca': 'no', 'cells_0': 12, 'cells_1': 92, 'nodes_0': 283, 'dropout_0': 0.1, 'nodes_1': 298, 'dropout_1': 0.1, 'nodes_2': 96, 'dropout_2': 0.1}. Best is trial 4 with value: 0.07865999999999995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Arrays: (200000, 100)\n",
      "Best Pred Obs: (200000,)\n",
      "Best_Ports_Len =  200000\n",
      "Values Preds: (200000,)\n",
      "Above thresh: (200000,)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "391/391 [==============================] - 6s 11ms/step - loss: 154.7138 - mae: 8.5974 - val_loss: 93.3794 - val_mae: 6.8467\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 89.0869 - mae: 6.6662 - val_loss: 83.4819 - val_mae: 6.4177\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 79.3646 - mae: 6.1827 - val_loss: 74.7660 - val_mae: 5.9423\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 72.6293 - mae: 5.7952 - val_loss: 69.9697 - val_mae: 5.6593\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 69.3547 - mae: 5.6081 - val_loss: 67.9803 - val_mae: 5.5815\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 67.6708 - mae: 5.5391 - val_loss: 66.1939 - val_mae: 5.5027\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 66.3525 - mae: 5.5025 - val_loss: 64.9680 - val_mae: 5.4544\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 65.4038 - mae: 5.4802 - val_loss: 65.0622 - val_mae: 5.5204\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 64.7927 - mae: 5.4585 - val_loss: 64.1573 - val_mae: 5.4970\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 64.3912 - mae: 5.4439 - val_loss: 63.6693 - val_mae: 5.4492\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 64.0896 - mae: 5.4295 - val_loss: 63.3976 - val_mae: 5.4433\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 63.8749 - mae: 5.4169 - val_loss: 63.3050 - val_mae: 5.4432\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 63.6991 - mae: 5.4045 - val_loss: 62.9489 - val_mae: 5.3516\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 63.5638 - mae: 5.3966 - val_loss: 63.2246 - val_mae: 5.3310\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 63.4459 - mae: 5.3884 - val_loss: 62.9856 - val_mae: 5.4361\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 63.3517 - mae: 5.3822 - val_loss: 64.0543 - val_mae: 5.3035\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 63.2683 - mae: 5.3755 - val_loss: 62.6554 - val_mae: 5.3794\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 63.1970 - mae: 5.3716 - val_loss: 62.5342 - val_mae: 5.3518\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 63.1416 - mae: 5.3669 - val_loss: 62.5601 - val_mae: 5.3274\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 63.0768 - mae: 5.3637 - val_loss: 62.5222 - val_mae: 5.3681\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 63.0334 - mae: 5.3602 - val_loss: 62.5913 - val_mae: 5.4125\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.9825 - mae: 5.3577 - val_loss: 62.4487 - val_mae: 5.3157\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.9518 - mae: 5.3551 - val_loss: 62.3473 - val_mae: 5.3625\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.9147 - mae: 5.3527 - val_loss: 62.3386 - val_mae: 5.3149\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.8946 - mae: 5.3508 - val_loss: 62.5167 - val_mae: 5.2872\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 62.8404 - mae: 5.3483 - val_loss: 62.3906 - val_mae: 5.3911\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.8196 - mae: 5.3473 - val_loss: 62.2412 - val_mae: 5.3607\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.7924 - mae: 5.3456 - val_loss: 62.3998 - val_mae: 5.2820\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.7824 - mae: 5.3444 - val_loss: 62.4231 - val_mae: 5.4058\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 62.7520 - mae: 5.3428 - val_loss: 62.2510 - val_mae: 5.2827\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.7202 - mae: 5.3412 - val_loss: 62.6260 - val_mae: 5.4546\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.7165 - mae: 5.3405 - val_loss: 62.2533 - val_mae: 5.3848\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.6918 - mae: 5.3391 - val_loss: 62.1202 - val_mae: 5.3018\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.6798 - mae: 5.3384 - val_loss: 62.1507 - val_mae: 5.2945\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.6667 - mae: 5.3377 - val_loss: 62.1139 - val_mae: 5.3498\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.6343 - mae: 5.3358 - val_loss: 62.0418 - val_mae: 5.3104\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.6273 - mae: 5.3354 - val_loss: 62.0334 - val_mae: 5.3024\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.6056 - mae: 5.3342 - val_loss: 62.1578 - val_mae: 5.2713\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.6019 - mae: 5.3340 - val_loss: 62.0908 - val_mae: 5.3634\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 62.5970 - mae: 5.3331 - val_loss: 62.0002 - val_mae: 5.3107\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.5676 - mae: 5.3319 - val_loss: 62.0035 - val_mae: 5.3404\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.5577 - mae: 5.3311 - val_loss: 62.0195 - val_mae: 5.2836\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.5411 - mae: 5.3300 - val_loss: 61.9508 - val_mae: 5.3293\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.5379 - mae: 5.3296 - val_loss: 61.9950 - val_mae: 5.3050\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.5292 - mae: 5.3290 - val_loss: 61.9810 - val_mae: 5.3420\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.5107 - mae: 5.3283 - val_loss: 61.9879 - val_mae: 5.3325\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.5094 - mae: 5.3278 - val_loss: 61.9845 - val_mae: 5.3450\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.5108 - mae: 5.3279 - val_loss: 62.0247 - val_mae: 5.3488\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.4808 - mae: 5.3263 - val_loss: 62.0958 - val_mae: 5.3906\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 62.4724 - mae: 5.3259 - val_loss: 61.9022 - val_mae: 5.3312\n",
      "98/98 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 14:32:40,030] Trial 5 finished with value: 0.07785500000000001 and parameters: {'activation': 'linear', 'cnn': 0, 'filters': 74, 'kernel_size': 2, 'lr': 0.0003725520608951676, 'dense': 0, 'lstm': 2, 'pool': 'max', 'optimizer': 'sgd', 'scaler': 'none', 'pca': 'no', 'cells_0': 89, 'cells_1': 52}. Best is trial 5 with value: 0.07785500000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Arrays: (200000, 100)\n",
      "Best Pred Obs: (200000,)\n",
      "Best_Ports_Len =  200000\n",
      "Values Preds: (200000,)\n",
      "Above thresh: (200000,)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "391/391 [==============================] - 4s 7ms/step - loss: 199.0360 - mae: 9.9446 - val_loss: 195.3184 - val_mae: 9.8131\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 191.1417 - mae: 9.6192 - val_loss: 182.8056 - val_mae: 9.3123\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 173.8121 - mae: 8.9600 - val_loss: 161.6708 - val_mae: 8.5403\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 150.9597 - mae: 8.1749 - val_loss: 138.7463 - val_mae: 7.7996\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 129.7838 - mae: 7.5395 - val_loss: 120.3117 - val_mae: 7.2898\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 114.3024 - mae: 7.1437 - val_loss: 107.7739 - val_mae: 6.9929\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 103.8340 - mae: 6.9075 - val_loss: 99.0661 - val_mae: 6.8018\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 96.2580 - mae: 6.7415 - val_loss: 92.4845 - val_mae: 6.6558\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 90.4006 - mae: 6.6065 - val_loss: 87.2981 - val_mae: 6.5292\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 85.7097 - mae: 6.4809 - val_loss: 83.0630 - val_mae: 6.4036\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 81.8102 - mae: 6.3523 - val_loss: 79.4784 - val_mae: 6.2703\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 78.4861 - mae: 6.2173 - val_loss: 76.4130 - val_mae: 6.1351\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 75.6623 - mae: 6.0837 - val_loss: 73.8370 - val_mae: 6.0044\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 73.3301 - mae: 5.9597 - val_loss: 71.7517 - val_mae: 5.8879\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 71.4754 - mae: 5.8524 - val_loss: 70.1236 - val_mae: 5.7898\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 70.0406 - mae: 5.7644 - val_loss: 68.8719 - val_mae: 5.7134\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 68.9260 - mae: 5.6964 - val_loss: 67.8823 - val_mae: 5.6512\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 68.0208 - mae: 5.6419 - val_loss: 67.0542 - val_mae: 5.6033\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 67.2378 - mae: 5.5978 - val_loss: 66.3165 - val_mae: 5.5637\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 66.5255 - mae: 5.5598 - val_loss: 65.6361 - val_mae: 5.5265\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 65.8665 - mae: 5.5251 - val_loss: 65.0067 - val_mae: 5.4930\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 65.2602 - mae: 5.4938 - val_loss: 64.4321 - val_mae: 5.4627\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 64.7147 - mae: 5.4644 - val_loss: 63.9239 - val_mae: 5.4402\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 64.2393 - mae: 5.4409 - val_loss: 63.4871 - val_mae: 5.4140\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 63.8386 - mae: 5.4189 - val_loss: 63.1266 - val_mae: 5.3992\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 63.5129 - mae: 5.4026 - val_loss: 62.8373 - val_mae: 5.3823\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 63.2566 - mae: 5.3893 - val_loss: 62.6136 - val_mae: 5.3706\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 63.0598 - mae: 5.3791 - val_loss: 62.4436 - val_mae: 5.3627\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.9108 - mae: 5.3713 - val_loss: 62.3142 - val_mae: 5.3560\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.7974 - mae: 5.3656 - val_loss: 62.2151 - val_mae: 5.3496\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.7096 - mae: 5.3605 - val_loss: 62.1373 - val_mae: 5.3428\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.6398 - mae: 5.3558 - val_loss: 62.0749 - val_mae: 5.3421\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.5823 - mae: 5.3522 - val_loss: 62.0219 - val_mae: 5.3374\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.5342 - mae: 5.3491 - val_loss: 61.9778 - val_mae: 5.3326\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.4927 - mae: 5.3459 - val_loss: 61.9392 - val_mae: 5.3289\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.4566 - mae: 5.3429 - val_loss: 61.9052 - val_mae: 5.3285\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.4244 - mae: 5.3405 - val_loss: 61.8759 - val_mae: 5.3275\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.3959 - mae: 5.3382 - val_loss: 61.8481 - val_mae: 5.3236\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.3702 - mae: 5.3359 - val_loss: 61.8247 - val_mae: 5.3241\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.3471 - mae: 5.3341 - val_loss: 61.8030 - val_mae: 5.3214\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.3261 - mae: 5.3323 - val_loss: 61.7824 - val_mae: 5.3179\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.3069 - mae: 5.3302 - val_loss: 61.7646 - val_mae: 5.3187\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.2896 - mae: 5.3293 - val_loss: 61.7481 - val_mae: 5.3121\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.2739 - mae: 5.3270 - val_loss: 61.7329 - val_mae: 5.3119\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.2595 - mae: 5.3258 - val_loss: 61.7191 - val_mae: 5.3111\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.2463 - mae: 5.3245 - val_loss: 61.7062 - val_mae: 5.3096\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.2342 - mae: 5.3230 - val_loss: 61.6947 - val_mae: 5.3084\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.2231 - mae: 5.3219 - val_loss: 61.6844 - val_mae: 5.3083\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.2132 - mae: 5.3206 - val_loss: 61.6745 - val_mae: 5.3073\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 62.2039 - mae: 5.3197 - val_loss: 61.6651 - val_mae: 5.3057\n",
      "98/98 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 14:34:41,090] Trial 6 finished with value: 0.07838 and parameters: {'activation': 'relu', 'cnn': 0, 'filters': 275, 'kernel_size': 3, 'lr': 0.0001255035002630316, 'dense': 0, 'lstm': 1, 'pool': 'max', 'optimizer': 'adam', 'scaler': 'std', 'pca': 'no', 'cells_0': 40}. Best is trial 5 with value: 0.07785500000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Arrays: (200000, 100)\n",
      "Best Pred Obs: (200000,)\n",
      "Best_Ports_Len =  200000\n",
      "Values Preds: (200000,)\n",
      "Above thresh: (200000,)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "391/391 [==============================] - 7s 13ms/step - loss: 200.6108 - mae: 10.0134 - val_loss: 199.3378 - val_mae: 9.9871\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.5773 - mae: 10.0118 - val_loss: 199.3028 - val_mae: 9.9854\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.5423 - mae: 10.0100 - val_loss: 199.2672 - val_mae: 9.9836\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.5070 - mae: 10.0083 - val_loss: 199.2312 - val_mae: 9.9818\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.4715 - mae: 10.0065 - val_loss: 199.1953 - val_mae: 9.9800\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.4358 - mae: 10.0047 - val_loss: 199.1593 - val_mae: 9.9782\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.4001 - mae: 10.0029 - val_loss: 199.1231 - val_mae: 9.9764\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.3644 - mae: 10.0012 - val_loss: 199.0869 - val_mae: 9.9746\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.3285 - mae: 9.9994 - val_loss: 199.0505 - val_mae: 9.9728\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.2926 - mae: 9.9976 - val_loss: 199.0141 - val_mae: 9.9709\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.2566 - mae: 9.9958 - val_loss: 198.9775 - val_mae: 9.9691\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.2204 - mae: 9.9940 - val_loss: 198.9407 - val_mae: 9.9673\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.1840 - mae: 9.9922 - val_loss: 198.9038 - val_mae: 9.9654\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.1476 - mae: 9.9904 - val_loss: 198.8668 - val_mae: 9.9636\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.1110 - mae: 9.9885 - val_loss: 198.8296 - val_mae: 9.9617\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.0741 - mae: 9.9867 - val_loss: 198.7922 - val_mae: 9.9599\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.0373 - mae: 9.9849 - val_loss: 198.7546 - val_mae: 9.9580\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 200.0000 - mae: 9.9830 - val_loss: 198.7169 - val_mae: 9.9561\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.9626 - mae: 9.9812 - val_loss: 198.6788 - val_mae: 9.9542\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.9249 - mae: 9.9793 - val_loss: 198.6405 - val_mae: 9.9523\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.8872 - mae: 9.9774 - val_loss: 198.6020 - val_mae: 9.9504\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.8491 - mae: 9.9755 - val_loss: 198.5631 - val_mae: 9.9484\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.8104 - mae: 9.9736 - val_loss: 198.5241 - val_mae: 9.9465\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.7719 - mae: 9.9717 - val_loss: 198.4846 - val_mae: 9.9445\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.7327 - mae: 9.9697 - val_loss: 198.4449 - val_mae: 9.9426\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.6934 - mae: 9.9678 - val_loss: 198.4049 - val_mae: 9.9406\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.6538 - mae: 9.9658 - val_loss: 198.3645 - val_mae: 9.9385\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.6138 - mae: 9.9638 - val_loss: 198.3237 - val_mae: 9.9365\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.5733 - mae: 9.9618 - val_loss: 198.2825 - val_mae: 9.9345\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.5324 - mae: 9.9598 - val_loss: 198.2408 - val_mae: 9.9324\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.4914 - mae: 9.9578 - val_loss: 198.1989 - val_mae: 9.9303\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.4496 - mae: 9.9557 - val_loss: 198.1563 - val_mae: 9.9282\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.4076 - mae: 9.9536 - val_loss: 198.1133 - val_mae: 9.9261\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.3649 - mae: 9.9515 - val_loss: 198.0699 - val_mae: 9.9239\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 199.3219 - mae: 9.9494 - val_loss: 198.0258 - val_mae: 9.9217\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.2781 - mae: 9.9472 - val_loss: 197.9813 - val_mae: 9.9195\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.2339 - mae: 9.9450 - val_loss: 197.9361 - val_mae: 9.9173\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.1892 - mae: 9.9428 - val_loss: 197.8903 - val_mae: 9.9150\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.1436 - mae: 9.9406 - val_loss: 197.8438 - val_mae: 9.9127\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.0977 - mae: 9.9383 - val_loss: 197.7967 - val_mae: 9.9104\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.0510 - mae: 9.9360 - val_loss: 197.7490 - val_mae: 9.9080\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 199.0038 - mae: 9.9336 - val_loss: 197.7004 - val_mae: 9.9056\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 198.9554 - mae: 9.9312 - val_loss: 197.6511 - val_mae: 9.9031\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 198.9067 - mae: 9.9288 - val_loss: 197.6010 - val_mae: 9.9007\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 198.8570 - mae: 9.9264 - val_loss: 197.5502 - val_mae: 9.8981\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 198.8065 - mae: 9.9239 - val_loss: 197.4985 - val_mae: 9.8956\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 198.7550 - mae: 9.9214 - val_loss: 197.4457 - val_mae: 9.8930\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 198.7027 - mae: 9.9188 - val_loss: 197.3921 - val_mae: 9.8903\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 198.6493 - mae: 9.9161 - val_loss: 197.3375 - val_mae: 9.8876\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 198.5953 - mae: 9.9135 - val_loss: 197.2818 - val_mae: 9.8849\n",
      "98/98 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 14:38:37,707] Trial 7 finished with value: 0.08381499999999997 and parameters: {'activation': 'relu', 'cnn': 1, 'filters': 235, 'kernel_size': 3, 'lr': 1.3040911610025205e-05, 'dense': 3, 'lstm': 2, 'pool': 'avg', 'optimizer': 'sgd', 'scaler': 'std', 'pca': 'yes', 'cells_0': 48, 'cells_1': 29, 'nodes_0': 252, 'dropout_0': 0.2, 'nodes_1': 146, 'dropout_1': 0.4, 'nodes_2': 191, 'dropout_2': 0.1}. Best is trial 5 with value: 0.07785500000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Arrays: (200000, 100)\n",
      "Best Pred Obs: (200000,)\n",
      "Best_Ports_Len =  200000\n",
      "Values Preds: (200000,)\n",
      "Above thresh: (200000,)\n",
      "Epoch 1/50\n",
      "391/391 [==============================] - 5s 7ms/step - loss: 200.3465 - mae: 10.0012 - val_loss: 198.8785 - val_mae: 9.9651\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 199.8912 - mae: 9.9787 - val_loss: 198.3984 - val_mae: 9.9414\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 199.3865 - mae: 9.9539 - val_loss: 197.8629 - val_mae: 9.9149\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 198.8159 - mae: 9.9258 - val_loss: 197.2479 - val_mae: 9.8846\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 198.1484 - mae: 9.8931 - val_loss: 196.5132 - val_mae: 9.8485\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 197.3335 - mae: 9.8534 - val_loss: 195.5971 - val_mae: 9.8037\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 196.2868 - mae: 9.8027 - val_loss: 194.3880 - val_mae: 9.7449\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 194.8598 - mae: 9.7341 - val_loss: 192.6796 - val_mae: 9.6625\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 192.7546 - mae: 9.6343 - val_loss: 190.0567 - val_mae: 9.5376\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 189.3575 - mae: 9.4762 - val_loss: 185.6017 - val_mae: 9.3296\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 183.2692 - mae: 9.2031 - val_loss: 177.1515 - val_mae: 8.9500\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 171.4746 - mae: 8.7137 - val_loss: 160.2546 - val_mae: 8.2536\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 151.1969 - mae: 8.0142 - val_loss: 133.3482 - val_mae: 7.3547\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 130.9457 - mae: 7.5730 - val_loss: 112.7489 - val_mae: 6.9410\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 122.2365 - mae: 7.5573 - val_loss: 105.5154 - val_mae: 6.9111\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 119.1602 - mae: 7.5771 - val_loss: 103.1568 - val_mae: 6.9221\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 117.1750 - mae: 7.5576 - val_loss: 101.9531 - val_mae: 6.9293\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 115.5217 - mae: 7.5268 - val_loss: 101.0615 - val_mae: 6.9343\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 114.1730 - mae: 7.4990 - val_loss: 100.3040 - val_mae: 6.9372\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 112.7832 - mae: 7.4702 - val_loss: 99.6113 - val_mae: 6.9380\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 111.5748 - mae: 7.4428 - val_loss: 98.9437 - val_mae: 6.9371\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 110.4935 - mae: 7.4204 - val_loss: 98.3321 - val_mae: 6.9329\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 109.4580 - mae: 7.3939 - val_loss: 97.7102 - val_mae: 6.9283\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 108.5500 - mae: 7.3718 - val_loss: 97.1393 - val_mae: 6.9206\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 107.6537 - mae: 7.3482 - val_loss: 96.5855 - val_mae: 6.9124\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 106.8412 - mae: 7.3259 - val_loss: 96.0719 - val_mae: 6.9032\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 106.1234 - mae: 7.3057 - val_loss: 95.6067 - val_mae: 6.8937\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 105.4504 - mae: 7.2850 - val_loss: 95.1815 - val_mae: 6.8850\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 104.8517 - mae: 7.2664 - val_loss: 94.8084 - val_mae: 6.8770\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 104.3812 - mae: 7.2508 - val_loss: 94.4942 - val_mae: 6.8700\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 103.8265 - mae: 7.2329 - val_loss: 94.2121 - val_mae: 6.8653\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 103.4702 - mae: 7.2216 - val_loss: 93.9893 - val_mae: 6.8608\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 103.0622 - mae: 7.2095 - val_loss: 93.7935 - val_mae: 6.8579\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 102.7412 - mae: 7.1990 - val_loss: 93.6305 - val_mae: 6.8559\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 102.4838 - mae: 7.1915 - val_loss: 93.4959 - val_mae: 6.8542\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 102.2251 - mae: 7.1842 - val_loss: 93.3833 - val_mae: 6.8531\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 101.9044 - mae: 7.1754 - val_loss: 93.2653 - val_mae: 6.8537\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 101.6909 - mae: 7.1695 - val_loss: 93.1764 - val_mae: 6.8534\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 101.4665 - mae: 7.1634 - val_loss: 93.0942 - val_mae: 6.8537\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 101.2419 - mae: 7.1578 - val_loss: 93.0189 - val_mae: 6.8542\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 101.0307 - mae: 7.1526 - val_loss: 92.9388 - val_mae: 6.8556\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 100.8517 - mae: 7.1497 - val_loss: 92.8775 - val_mae: 6.8560\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 100.6828 - mae: 7.1442 - val_loss: 92.8229 - val_mae: 6.8563\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 100.4766 - mae: 7.1400 - val_loss: 92.7664 - val_mae: 6.8572\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 100.3277 - mae: 7.1355 - val_loss: 92.6933 - val_mae: 6.8592\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 100.1258 - mae: 7.1322 - val_loss: 92.6417 - val_mae: 6.8598\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 99.9994 - mae: 7.1294 - val_loss: 92.6051 - val_mae: 6.8594\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 99.7993 - mae: 7.1243 - val_loss: 92.5502 - val_mae: 6.8608\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 99.6342 - mae: 7.1209 - val_loss: 92.4927 - val_mae: 6.8626\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 99.4539 - mae: 7.1171 - val_loss: 92.4357 - val_mae: 6.8645\n",
      "98/98 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 14:40:25,176] Trial 8 finished with value: 0.11848999999999998 and parameters: {'activation': 'tanh', 'cnn': 0, 'filters': 259, 'kernel_size': 1, 'lr': 0.00018792949714907796, 'dense': 2, 'lstm': 1, 'pool': 'max', 'optimizer': 'sgd', 'scaler': 'std', 'pca': 'yes', 'cells_0': 3, 'nodes_0': 10, 'dropout_0': 0.5, 'nodes_1': 172, 'dropout_1': 0.5}. Best is trial 5 with value: 0.07785500000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Arrays: (200000, 100)\n",
      "Best Pred Obs: (200000,)\n",
      "Best_Ports_Len =  200000\n",
      "Values Preds: (200000,)\n",
      "Above thresh: (200000,)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "391/391 [==============================] - 5s 8ms/step - loss: 200.5127 - mae: 10.0085 - val_loss: 199.0990 - val_mae: 9.9750\n",
      "Epoch 2/50\n",
      "333/391 [========================>.....] - ETA: 0s - loss: 200.0055 - mae: 9.9856"
     ]
    }
   ],
   "source": [
    "# Run optimization\n",
    "study_name = \"study_\"+project_name\n",
    "storage_name = 'sqlite:///{}.db'.format(study_name)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name, load_if_exists=True) # reducing the loss and the complexity\n",
    "print(f\"Sampler is {study.sampler.__class__.__name__}\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params: \", study.best_params)\n",
    "print(\"Best value: \", study.best_value)\n",
    "print(\"Best Trial: \", study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_5 = {'activation': 'sigmoid', 'cnn': 0, 'filters': 31, 'kernel_size': 4, 'lr': 0.0007075516779091285, 'dense': 0, 'lstm': 1, 'pool': 'avg', 'optimizer': 'sgd', 'scaler': 'minmax', 'pca': 'no', 'cells_0': 84}\n",
    "\n",
    "params_teste = {'activation': 'sigmoid', 'cnn': 0, 'filters': 486, 'kernel_size': 4, 'lr': 0.03717307353131541, 'dense': 3, 'lstm': 3, 'pool': 'avg', 'optimizer': 'nadam', 'scaler': 'none', 'pca': 'yes', 'cells_0': 15, 'cells_1': 33, 'cells_2': 35, 'nodes_0': 120, 'dropout_0': 0.5, 'nodes_1': 76, 'dropout_1': 0.5, 'nodes_2': 112, 'dropout_2': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(params, X_train_, X_val_, epochs=50, batch_size=2048):\n",
    "    \n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    clear_session()    \n",
    "\n",
    "    activation = params['activation']\n",
    "    cnn = params['cnn']\n",
    "    filters = params['filters']\n",
    "    kernel_size = params['kernel_size']\n",
    "    learning_rate = params['lr']\n",
    "    dense = params['dense']\n",
    "    lstm = params['lstm']\n",
    "    \n",
    "    pool = params['pool']\n",
    "    if(pool == 'max'):\n",
    "        pool_layer = MaxPooling1D(pool_size=1)\n",
    "    else:\n",
    "        pool_layer = AveragePooling1D(pool_size=1)\n",
    "\n",
    "    optimizer = params['optimizer']\n",
    "    if optimizer=='adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer=='nadam':\n",
    "        opt = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "    elif optimizer=='sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    scaler = params['scaler']\n",
    "    if(scaler == 'std'):\n",
    "        scl = StandardScaler()\n",
    "    else:\n",
    "        scl = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    if(scaler=='std' or scaler=='minmax'):\n",
    "        X_train__ = scl.fit_transform(X_train_)\n",
    "        X_val__ = scl.transform(X_val_)\n",
    "    else:\n",
    "        X_train__ = X_train_\n",
    "        X_val__ = X_val_\n",
    "\n",
    "    pca = params['pca']\n",
    "    if(pca == 'yes'):\n",
    "        pCA = PCA()\n",
    "        X_train___ = pCA.fit_transform(X_train__)\n",
    "        X_val___ = pCA.transform(X_val__)\n",
    "    else:\n",
    "        X_train___ = X_train__\n",
    "        X_val___ = X_val__\n",
    "\n",
    "    del X_train__\n",
    "    del X_val__\n",
    "\n",
    "    if(cnn>0):\n",
    "        X_train = X_train___.reshape((X_train___.shape[0], n_seq, n_steps, n_features))\n",
    "        X_val = X_val___.reshape((X_val___.shape[0], n_seq, n_steps, n_features))\n",
    "    else:\n",
    "        X_train = X_train___.reshape((X_train___.shape[0], n_steps, n_features))\n",
    "        X_val = X_val___.reshape((X_val___.shape[0], n_steps, n_features))\n",
    "\n",
    "    del X_train___\n",
    "    del X_val___\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    if(cnn>0):\n",
    "        sp=(None, n_steps, n_features)\n",
    "    else:\n",
    "        sp=(n_steps, n_features)\n",
    "    model.add(Input(shape=sp))\n",
    "    \n",
    "    if(cnn>0):\n",
    "        model.add(TimeDistributed(Conv1D(filters=filters, kernel_size=kernel_size, padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(pool_layer))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    for i in range(lstm):\n",
    "        cells = params['cells_'+str(i)]\n",
    "        if(lstm > 1 and i < lstm-1):\n",
    "            return_sequences=True\n",
    "        else:\n",
    "            return_sequences=False\n",
    "        model.add(LSTM(cells, activation=activation, return_sequences=return_sequences))\n",
    "    for i in range(dense):\n",
    "        nodes = params['nodes_'+str(i)]\n",
    "        model.add(Dense(nodes, activation='relu'))\n",
    "        dropout = params['dropout_'+str(i)]\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(100, activation='linear'))\n",
    "    \n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=opt,\n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    es_cb = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)    \n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=2048, epochs=40, verbose=True, callbacks=[es_cb])\n",
    "    \n",
    "    y_val_pred = model.predict(X_val, batch_size=2048)\n",
    "    \n",
    "    op = getOp(Xindices, y_val_pred, y_val)\n",
    "\n",
    "    return model, op, y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/40\n",
      "391/391 [==============================] - 11s 20ms/step - loss: 101.9299 - mae: 7.2363 - val_loss: 89.9587 - val_mae: 6.9026\n",
      "Epoch 2/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 92.7634 - mae: 7.0151 - val_loss: 90.4387 - val_mae: 6.8435\n",
      "Epoch 3/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 91.4830 - mae: 6.9607 - val_loss: 88.3596 - val_mae: 6.7886\n",
      "Epoch 4/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 89.2013 - mae: 6.8550 - val_loss: 85.4807 - val_mae: 6.6853\n",
      "Epoch 5/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 87.5356 - mae: 6.7936 - val_loss: 85.0293 - val_mae: 6.6452\n",
      "Epoch 6/40\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 85.4561 - mae: 6.7049 - val_loss: 82.1191 - val_mae: 6.5302\n",
      "Epoch 7/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 83.3919 - mae: 6.6162 - val_loss: 81.2195 - val_mae: 6.4644\n",
      "Epoch 8/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 81.5919 - mae: 6.5383 - val_loss: 77.1592 - val_mae: 6.3234\n",
      "Epoch 9/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 79.8898 - mae: 6.4665 - val_loss: 78.2608 - val_mae: 6.4097\n",
      "Epoch 10/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 78.6797 - mae: 6.4211 - val_loss: 76.2812 - val_mae: 6.2255\n",
      "Epoch 11/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 78.2890 - mae: 6.4034 - val_loss: 74.4294 - val_mae: 6.2122\n",
      "Epoch 12/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 77.7843 - mae: 6.3816 - val_loss: 74.0996 - val_mae: 6.1574\n",
      "Epoch 13/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 77.5449 - mae: 6.3697 - val_loss: 74.3413 - val_mae: 6.1860\n",
      "Epoch 14/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 77.2528 - mae: 6.3545 - val_loss: 74.2560 - val_mae: 6.1616\n",
      "Epoch 15/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 77.0026 - mae: 6.3407 - val_loss: 74.4263 - val_mae: 6.1466\n",
      "Epoch 16/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.9368 - mae: 6.3327 - val_loss: 74.6638 - val_mae: 6.1942\n",
      "Epoch 17/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.8319 - mae: 6.3309 - val_loss: 74.1606 - val_mae: 6.1572\n",
      "Epoch 18/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.7475 - mae: 6.3289 - val_loss: 77.0789 - val_mae: 6.2082\n",
      "Epoch 19/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.6403 - mae: 6.3240 - val_loss: 73.6734 - val_mae: 6.1868\n",
      "Epoch 20/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.5007 - mae: 6.3159 - val_loss: 76.1354 - val_mae: 6.2116\n",
      "Epoch 21/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.5920 - mae: 6.3203 - val_loss: 75.4236 - val_mae: 6.1487\n",
      "Epoch 22/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.4585 - mae: 6.3150 - val_loss: 74.1570 - val_mae: 6.1289\n",
      "Epoch 23/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.3474 - mae: 6.3080 - val_loss: 74.4963 - val_mae: 6.1419\n",
      "Epoch 24/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.4630 - mae: 6.3115 - val_loss: 73.5674 - val_mae: 6.0862\n",
      "Epoch 25/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.2291 - mae: 6.3007 - val_loss: 75.7642 - val_mae: 6.1460\n",
      "Epoch 26/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.3382 - mae: 6.3072 - val_loss: 74.1300 - val_mae: 6.1235\n",
      "Epoch 27/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.4122 - mae: 6.3100 - val_loss: 75.7284 - val_mae: 6.1631\n",
      "Epoch 28/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.3319 - mae: 6.3082 - val_loss: 74.4724 - val_mae: 6.1778\n",
      "Epoch 29/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.2441 - mae: 6.3036 - val_loss: 74.0814 - val_mae: 6.1417\n",
      "Epoch 30/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.2554 - mae: 6.3059 - val_loss: 73.3147 - val_mae: 6.1395\n",
      "Epoch 31/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.3518 - mae: 6.3066 - val_loss: 75.5796 - val_mae: 6.1565\n",
      "Epoch 32/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.1602 - mae: 6.2993 - val_loss: 74.0852 - val_mae: 6.1570\n",
      "Epoch 33/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.2809 - mae: 6.3071 - val_loss: 74.2041 - val_mae: 6.1625\n",
      "Epoch 34/40\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 76.2020 - mae: 6.3002 - val_loss: 77.0751 - val_mae: 6.2049\n",
      "Epoch 35/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.3049 - mae: 6.3066 - val_loss: 74.3907 - val_mae: 6.1430\n",
      "Epoch 36/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.3690 - mae: 6.3118 - val_loss: 73.8898 - val_mae: 6.1066\n",
      "Epoch 37/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.5903 - mae: 6.3229 - val_loss: 75.9486 - val_mae: 6.1666\n",
      "Epoch 38/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 77.0837 - mae: 6.3522 - val_loss: 74.2162 - val_mae: 6.1422\n",
      "Epoch 39/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.6793 - mae: 6.3311 - val_loss: 75.9621 - val_mae: 6.2051\n",
      "Epoch 40/40\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 76.7710 - mae: 6.3325 - val_loss: 75.7904 - val_mae: 6.1994\n",
      "98/98 [==============================] - 1s 2ms/step\n",
      "Best Predicted Values: (200000,)\n",
      "Observed values: (200000, 100)\n",
      "Max Arrays: (200000, 100)\n",
      "Best Pred Obs: (200000,)\n",
      "Best_Ports_Len =  200000\n",
      "Values Preds: (200000,)\n",
      "Above thresh: (200000,)\n"
     ]
    }
   ],
   "source": [
    "model, op, y_val_pred = getModel(params_teste, X_train_, X_val_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11848999999999998\n"
     ]
    }
   ],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_val_pred.shape)\n",
    "y_val_pred.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame.from_records(y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_\"+project_name\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(1):\n",
    "    try:\n",
    "        model.save(model_name, save_format=\"tf\")\n",
    "    except Exception:\n",
    "        model.save(model_name+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_name, custom_objects={'macro_soft_f1':macro_soft_f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = loaded_model.predict(X_train)\n",
    "    \n",
    "y_val_pred = loaded_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_bin = np.where(y_train_pred < 0.5, 0, 1)\n",
    "y_val_pred_bin = np.where(y_val_pred < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_val = 0\n",
    "for line in range(y_val.shape[0]):\n",
    "    for column in range(y_val.shape[1]):\n",
    "\n",
    "        if(y_val_pred_bin[line, column] == y_val[line, column]):\n",
    "            acc_val += 1\n",
    "acc_val /= (5*y_val.shape[0])\n",
    "\n",
    "\n",
    "acc_train = 0\n",
    "for line in range(y_train.shape[0]):\n",
    "    for column in range(y_train.shape[1]):\n",
    "\n",
    "        if(y_train_pred_bin[line, column] == y_train[line, column]):\n",
    "            acc_train += 1\n",
    "acc_train /= (5*y_train.shape[0])\n",
    "\n",
    "print('Acc train:', acc_train)\n",
    "print('Acc val:', acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train=loaded_model.evaluate(X_train, y_train)\n",
    "score_val=loaded_model.evaluate(X_val, y_val)\n",
    "print(score_train)\n",
    "print(score_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_predicted_classes = np.argsort(y_val_pred, axis=1)[:, -5:]  # Índices das top 5 classes previstas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifique se a classe predita está entre as top 5 mais prováveis\n",
    "is_top_5 = np.any(top_5_predicted_classes == np.argsort(y_val, axis=1)[:, -5:], axis=1)\n",
    "\n",
    "# Calcule a acurácia top-5\n",
    "top_5_accuracy = np.sum(is_top_5) / len(y_val)\n",
    "\n",
    "print(f'A acurácia top-5 foi: {top_5_accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('./classical_channel_samples.csv', header=None)\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_teste = dados.iloc[:, :].to_numpy()\n",
    "\n",
    "train_df, dados_teste = train_test_split(dados_teste, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = []\n",
    "best_pred_obs_ports = []\n",
    "\n",
    "# for n in range(len(loaded_model)):\n",
    "for n in range(1):\n",
    "\n",
    "    #Criando um dataset para cada valor do N de entrada\n",
    "    x_data = []\n",
    "    observed_ports = []\n",
    "\n",
    "    \n",
    "    arr, indices = reduzir_array(dados_teste, number_of_ports)\n",
    "    x_data.append(arr)\n",
    "\n",
    "\n",
    "    x_data = np.array(x_data)\n",
    "\n",
    "    \n",
    "    predictions_dB = y_val_pred\n",
    "\n",
    "    # Após a predição do modelo, verificar se a escolha foi melhor que a referência (Max N_init portas observadas)\n",
    "    # Caso contrário, uma porta observada, tiver sido melhor que a predição, escolhe a porta observada\n",
    "\n",
    "    # Monta um array com o valor da porta observada\n",
    "    observed_values = np.zeros((len(dados_teste),100)) - 100\n",
    "    observed_values[:,indices] = dados_teste[:,indices]\n",
    "\n",
    "\n",
    "    # Monta um array com o valor real da porta preditada\n",
    "    pred_classes = np.argmax(predictions_dB, axis=1)\n",
    "\n",
    "    predicted_values = np.zeros((len(dados_teste),100)) - 100\n",
    "    predicted_values[np.arange(len(pred_classes)), pred_classes] = dados_teste[np.arange(len(pred_classes)), pred_classes]\n",
    "\n",
    "    # Pega o máximo entre as portas observadas e preditadas\n",
    "    maximo_entre_arrays = np.maximum(observed_values, predicted_values)\n",
    "\n",
    "    # Indice (Porta) da melhor predição/observada\n",
    "    best_pred_obs_port = np.argmax(maximo_entre_arrays, axis=1)\n",
    "\n",
    "\n",
    "    predicted_classes.append(np.argmax(predictions_dB, axis=1))\n",
    "    best_pred_obs_ports.append(best_pred_obs_port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OP LSTM2\n",
    "predicted_lstm_op = []\n",
    "\n",
    "# for i in range(len(1)):\n",
    "# Valor do dado real na porta escolhida pela predição\n",
    "values_for_predicted_classes = dados_teste[np.arange(len(dados_teste)), best_pred_obs_ports]\n",
    "\n",
    "# True or False para os valores que ficaram acima do limiar do gamma_th\n",
    "# acima_do_limiar = values_for_predicted_classes > gamma_th_linear\n",
    "# acima_do_limiar = values_for_predicted_classes > gamma_th\n",
    "acima_do_limiar = values_for_predicted_classes > 5\n",
    "\n",
    "# Probabilidade de Outage\n",
    "p_out = 1 - np.sum(acima_do_limiar) / len(dados_teste)\n",
    "print(f'A probabilidade de Outage (OP) foi {\"{:.5e}\".format(p_out)} = {\"{:.2e}\".format(p_out).replace(\"e\", \"*10^\")}\\n')\n",
    "\n",
    "predicted_lstm_op.append(p_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op=getOp(y_val, y_val_pred, number_of_ports)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM OP\n",
    "\n",
    "5  Portas - 0.23139500000000002\n",
    "\n",
    "6  Portas - 0.20364000000000004\n",
    "\n",
    "7  Portas - 0.18842000000000003\n",
    "\n",
    "8  Portas - 0.17806\n",
    "\n",
    "9  Portas - 0.17127499999999996\n",
    "\n",
    "10 Portas - 0.16713\n",
    "\n",
    "11 Portas - 0.16464999999999996\n",
    "\n",
    "13 Portas - 0.16351000000000004\n",
    "\n",
    "15 Portas - 0.16324499999999997\n",
    "\n",
    "20 Portas - 0.16295000000000004\n",
    "\n",
    "25 Portas - 0.16242999999999996\n",
    "\n",
    "25 Portas - 0.16235"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
